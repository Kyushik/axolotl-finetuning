import argparse
import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================================
# argparse ì¸ì ì •ì˜
# ================================
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True, help="ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ ì´ë¦„")
parser.add_argument(
    "--test_input",
    type=str,
    default="ë‚˜ ì¹œêµ¬ë‘ ë§‰ ì‹¸ì› ì–´",
    help="ëª¨ë¸ì— ì…ë ¥í•  ì‚¬ìš©ì í…ìŠ¤íŠ¸ (ê¸°ë³¸ê°’ ìˆìŒ)"
)
args = parser.parse_args()

# ================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ================================
tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    args.model_path,
    trust_remote_code=True,
    torch_dtype="auto",
    device_map={"": "cuda:0"},
)
model.eval()

# ================================
# ëª¨ë¸ ì¶œë ¥ ìƒì„± í•¨ìˆ˜
# ================================
def get_model_output(input_text, model, tokenizer):
    messages = [{"role": "user", "content": input_text}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.95,
            do_sample=True,
        )
    output_ids = outputs[0][inputs["input_ids"].shape[1]:]
    decoded = tokenizer.decode(output_ids, skip_special_tokens=False).strip()
    decoded = decoded.split("<|im_end|>")[0].strip()
    return decoded

# ================================
# ì¶”ë¡  ì‹¤í–‰
# ================================
print("\nğŸ” ëª¨ë¸ ì‘ë‹µ:")
print(get_model_output(args.test_input, model, tokenizer))