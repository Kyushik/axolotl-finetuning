import argparse
import json
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================================
# argparse ì¸ì ì •ì˜
# ================================
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True, help="ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ ì´ë¦„")
parser.add_argument(
    "--test_input",
    type=str,
    default="ì´ì ¹ì˜ê¸¸ë™ì˜ìŠˆë‹¨ì´ì‹ ì¸Œê·€ëª°á„’á†ì•¼íŒ”ë„ì˜íš¡á„’á†¡á†¼á„’á†ë˜ëŠ¥íˆì•Œá„Œá†¡ì—…á„‚á†á†«ì§€ë¼",
    help="ì¶”ë¡  ì˜ˆì‹œ ì…ë ¥ (ê¸°ë³¸ê°’ì€ ì˜› í•œê¸€ ë¬¸ì¥)"
)
parser.add_argument("--jsonl_path", type=str, default="honggildong_test.jsonl", help="í…ŒìŠ¤íŠ¸ jsonl ê²½ë¡œ")
args = parser.parse_args()

# ================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ================================
tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    args.model_path,
    trust_remote_code=True,
    torch_dtype="auto",
    device_map={"": "cuda:0"},
)
model.eval()

# ================================
# JSONL ë¶ˆëŸ¬ì˜¤ê¸°
# ================================
def load_jsonl(filename):
    with open(filename, "r", encoding="utf-8") as f:
        for line in f:
            yield json.loads(line)

# ================================
# ë‹¨ì–´ ì •ë‹µë¥  ê³„ì‚° í•¨ìˆ˜
# ================================
def get_word_overlap(pred, ref):
    ref_words = set(ref.strip().split())
    pred_words = set(pred.strip().split())
    common = ref_words & pred_words
    return len(common) / len(ref_words) if ref_words else 0

# ================================
# ëª¨ë¸ ì¶œë ¥ ìƒì„± í•¨ìˆ˜
# ================================
def get_model_output(input_text, model, tokenizer):
    prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
ì…ë ¥ì˜ ì˜› í•œê¸€ ë‚´ìš©ì„ í˜„ëŒ€ì˜ í•œê¸€ë¡œ ë³€í™˜í•´ì¤˜

### Input:
{input_text}

### Response:
"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.01,
            top_p=0.95,
            do_sample=True,
        )
    output_ids = outputs[0][inputs["input_ids"].shape[1]:]
    decoded = tokenizer.decode(output_ids, skip_special_tokens=False).strip()
    decoded = decoded.split("<|im_end|>")[0].strip()
    return decoded

# ================================
# í‰ê°€ ë£¨í”„
# ================================
results = []
for item in tqdm(load_jsonl(args.jsonl_path)):
    input_text = item["input"]
    target_text = item["output"]
    decoded = get_model_output(input_text, model, tokenizer)
    acc = get_word_overlap(decoded, target_text)
    results.append(acc)

accuracy = sum(results) / len(results)
print(f"\nâœ… ì „ì²´ ë‹¨ì–´ í¬í•¨ ê¸°ì¤€ Accuracy: {accuracy:.3f}")

# ================================
# ì¶”ë¡  ì˜ˆì‹œ (ì„ íƒì )
# ================================
if args.test_input:
    print("\nğŸ” ì¶”ë¡  ì˜ˆì‹œ:")
    print("Input:", args.test_input)
    print("Output:", get_model_output(args.test_input, model, tokenizer))
